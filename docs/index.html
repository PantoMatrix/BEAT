
<DOCTYPE html>
    <title>BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis</title>

    <meta charset="utf-8">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Project Page for BEAT">
    <meta name="author" content="haiyang">

    <meta property="og:image" content="https://pantomatrix.github.io/BEAT/assets/teaser2.png">
    <meta property="og:url" content="https://pantomatrix.github.io/BEAT/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis">
    <meta property="og:video" content="https://www.youtube.com/embed/F6nXVTUY0KQ">
    <meta property="og:description" content="BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis.">


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-JM2CPK6QLP"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-JM2CPK6QLP');
    </script>
</head>
    <style>
        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fCBc4EsA.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBxc4EsA.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fChc4EsA.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBBc4.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7mxKOzY.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4WxKOzY.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7GxKOzY.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4mxK.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }
    </style>

    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <body>
        <div class="container">
            <div class="row mb-2 mt-4" id="paper-title">
                <h1 class="col-md-12 text-center">
                    BEAT
                </h1>
                <h3 class="col-md-12 text-center">
                    A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis
                </h3>
                <h3 class="col-md-12 text-center">
                    <small>ECCV 2022</small>
                </h3>

            </div>

            <div class="row" id="authors">
                <div class="mx-auto text-center">
                    <ul class="list-inline mb-0">
                        <li class="list-inline-item">
                            <a href="https://h-liu1997.github.io/">Haiyang Liu</a><sup>1</sup>

                        <li class="list-inline-item">
                            <a href="https://zzhat0706.github.io/PersonalPage/">Zihao Zhu</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a href="https://iwanao731.github.io/">Naoya Iwamoto</a><sup>3</sup>

                        <li class="list-inline-item">
                            <a href="https://scholar.google.com/citations?user=9sWVrREAAAAJ&hl=en">Yichen Peng</a><sup>4</sup>
                        
                        <li class="list-inline-item">
                            <a href="https://scholar.google.co.jp/citations?user=hgCoNowAAAAJ&hl=ja">Zhengqing Li</a><sup>3</sup>

                        <li class="list-inline-item">
                            <a href="">You Zhou</a><sup>3</sup>

                        <li class="list-inline-item">
                            <a href="https://sites.google.com/site/bozkurtelif/">Elif Bozkurt</a><sup>3</sup>

                        <li class="list-inline-item">
                            <a href="http://www.bozheng-lab.com/">Bo Zheng</a><sup>3</sup>
                    </ul>
                    <p id="institution">
                        <sup>1</sup>The University of Tokyo &nbsp;&nbsp;&nbsp;&nbsp;
                        <sup>2</sup>Keio University &nbsp;&nbsp;&nbsp;&nbsp;
                        <sup>3</sup>Digital Human Lab, HuaWei Technologies &nbsp;&nbsp;&nbsp;&nbsp;
                        <sup>4</sup>Japan Advanced Institute of Science and Technology
                    </p>
                </div>
            </div>
            <div class="row mb-2" id="links">
                <div class="mx-auto">
                    <ul class="nav">
                        <li class="nav-item text-center">
                            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670605.pdf" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                                </svg><br>
                                Paper
                            </a>
                        </li>
                        <li class="nav-item text-center">
                            <a href="https://pantomatrix.github.io/BEAT-Dataset/" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M12,3C7.58,3 4,4.79 4,7C4,9.21 7.58,11 12,11C16.42,11 20,9.21 20,7C20,4.79 16.42,3 12,3M4,9V12C4,14.21 7.58,16 12,16C16.42,16 20,14.21 20,12V9C20,11.21 16.42,13 12,13C7.58,13 4,11.21 4,9M4,14V17C4,19.21 7.58,21 12,21C16.42,21 20,19.21 20,17V14C20,16.21 16.42,18 12,18C7.58,18 4,16.21 4,14Z" />
                                </svg><br>
                                Dataset
                            </a>
                        </li>
                        <li class="nav-item text-center">
                            <a href="https://github.com/PantoMatrix/BEAT" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z" />
                                </svg><br>
                                Code
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">
                    <div class="row no-gutters pb-2">
                        <div class="col-md-12">
                            <span></span><img src="assets/teaser2.png" class="img-responsive">
                        </div>
                    </div>
                    <p class="text-justify">
                        We present a new conversational gestures dataset (BEAT) with cascaded motion network (CaMN) model as a baseline for synthesis realistic, vivid and human-like conversational gestures.
                        BEAT contains 76-hour 3D motion from the motion capture system, paired with 52D facial blendshape weights, audio, text, semantic relevancy and emotion categories annotations. 
                    </p>

                </div>
            </div>
            <div class="row mb-4" id="overview-video">
                <div class="col-md-8 mx-auto grey-container">
                    <h4 class="pb-2"><strong>Paper Overview</strong></h4>
                    <div class="embed-responsive embed-responsive-16by9">
                        <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/F6nXVTUY0KQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
            <div class="row mb-4" id="overview-dataset">
                <div class="col-md-8 mx-auto grey-container">
                    <h4 class="pb-2"><strong>Semantic Relevancy Annotations</strong></h4>
                    <div class="row pt-1 pb-1">
                        <div class="col-12">
                            <video loop="" playsinline="" controls="" width="85%">
                                <source src="assets/semantic.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <br>
                    <p class="text-justify">
                        In order to develop and evaluate the semantic relevancy between gestures and speech content, we provide a score and category-label for each frame: no gestures (0), beat gestures (1), low-middle-high quaility deictic gestures (2-3-4), iconic gestures (5-6-7), metaphoric gestures (8-9-10). The defination of gesture categories is from <a href="https://files.eric.ed.gov/fulltext/ED573247.pdf" target="_blank">Four-Type-Gestures</a>. 
                        The annotation tool is based on <a href="https://www.robots.ox.ac.uk/~vgg/software/via/" target="_blank">VGG Image Annotator</a> and <a href="https://babel.is.tue.mpg.de/" target="_blank">BABEL</a>, and examples in the above video are in <a href="https://drive.google.com/file/d/1uroWlifFXGKUE-gv-HbPoU347DRsafBU/view?usp=share_link" target="_blank">Annotation Tools</a>.    
                    </p>
                </div>
            </div>

            <div class="row mb-4">
                <div class="col-md-8 mx-auto">
                    <h4><strong>Emotional Gestures</strong></h4>
                    <p class="text-justify">
                        Here we show raw captured gestures data in with 8 emotions for each speaker. The emotion categories are neutral, happiness, anger, sadness, contempt, surprise, fear and disgust. We also render all data in <a href="https://drive.google.com/drive/folders/1ghZ7_4LkCyM_IZxTElzAwPzGheLrBGBu?usp=share_link" target="_blank">All Renderd Videos</a>.
                    </p>
                    <div class="row pt-1 pb-1">
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/2_scott_neutral.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/2_scott_happiness.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/2_scott_anger.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/2_scott_sadness.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/2_scott_contempt.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/2_scott_surprise.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/2_scott_fear.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/2_scott_disgust.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mb-4">
                <div class="col-md-8 mx-auto">
                    <h4><strong>Multi-Language Data</strong></h4>
                    <p class="text-justify">
                        BEAT contains recording in English (60h), Chinese (16h), Spanish (2h) and Japanese (2h). All Non-Engish speakers have the English recording with the same text content for comparsions.
                    </p>
                    <div class="row pt-1 pb-1">
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/english.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/chinese.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/spanish.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/japanese.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>
            </div>

            <div class="row mb-4">
                <div class="col-md-8 mx-auto">
                    <h4><strong>30 Speakers with Self-Talk and Converstion Recording</strong></h4>
                    <p class="text-justify">
                        Half of BEAT dataset are Self-Talk (read predefined text) recording, which is proposed to explore the personality of different speakers with the same speech content. 
                        We list below a representative sample of six videos that include speakers of different ethnicities and genders. Each recording is one minuate with total 118 recordings.
                    </p>
                    <div class="row pt-1 pb-1">
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/21_ayana_9.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/3_solomon_9.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/10_kieks_0_9.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/1_wayne_0_9.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/5_catherine_0_9.mp4" type="video/mp4">
                            </video>
                        </div>
                        <div class="col-md-6">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/4_lawrence_0_9.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>

                    <p class="text-justify">
                        The other half of BEAT dataset are Converstion Recording (chat with director), Each recording is 10 minuates with total 12 recordings.
                    </p>
                    <div class="row pt-1 pb-1">
                        <div class="col-md-12">
                            <video class="results" loop="" playsinline="" controls="" width="85%" onloadstart="this.volume=0.9">
                                <source src="assets/11_nidal_1_11_11.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row mb-2">
                <div class="col-md-8 mx-auto">
                    <h4 class="mb-3">Bibtex</h4>
                    <div class="bibtex">@article{liu2022beat,
  title={BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis},
  author={Liu, Haiyang and Zhu, Zihao and Iwamoto, Naoya and Peng, Yichen and Li, Zhengqing and Zhou, You and Bozkurt, Elif and Zheng, Bo},
  journal={arXiv preprint arXiv:2203.05297},
  year={2022}}
            </div>
                </div>
            </div>
            <div class="row mb-3">
                <div class="col-md-8 mx-auto">
                    <h4><strong>More Thanks</strong></h4>
                    <p class="text-justify">
                        We thank Hailing Pi for communicating with the recording actors of the BEAT dataset.
                        The website is inspired by the template of <a href="https://alexyu.net/pixelnerf/" target="_blank">pixelnerf</a>.
                    </p>
                </div>
            </div>
            <div class="row mb-4" id="license">
                <div class="col-md-8 mx-auto grey-container">
                    <br>
                    <p class="text-justify">
                        Licensed under the Non-commercial license.
                    </p>
                </div>
            </div>
            
        </div> <!-- container -->
    </body>

</DOCTYPE>
